#!/usr/bin/python3
# -*- coding: utf-8 -*-
import os
import sys
import json
from time import time
from datetime import datetime

from flask import Flask, render_template, request, jsonify
from ollama import Client
import requests

# ========= Paths & Config =========
BASE_PATH = os.path.abspath("./")
CONFIG_PATH = os.path.join(BASE_PATH, "config", "config.json")
LOG_PATH = os.path.join(BASE_PATH, "log")
os.makedirs(LOG_PATH, exist_ok=True)

def _now():
    return datetime.now().strftime("%d/%m/%Y %H:%M:%S")

def log_error(msg: str):
    line = f"[{_now()}] [ERROR] {msg}"
    print(line, file=sys.stderr)
    try:
        with open(os.path.join(LOG_PATH, "error.txt"), "a", encoding="utf-8") as f:
            f.write(line + "\n")
    except Exception:
        pass

def log_info(msg: str):
    print(f"[{_now()}] [INFO] {msg}", file=sys.stderr)

# Caricamento config
if not os.path.exists(CONFIG_PATH):
    log_error(f"File di configurazione mancante: {CONFIG_PATH}")
    sys.exit(1)
try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        config = json.load(f)
except Exception as e:
    log_error(f"Impossibile leggere/parsare il config: {CONFIG_PATH} — {e}")
    sys.exit(1)

OLLAMA_BASE = config.get("ollama_host", "http://127.0.0.1:11434")
DEFAULT_MODEL = config.get("default_model", "llama3:latest")
PROMPT_SYSTEM = config.get("prompt_system",
                           "Sei E.V.A. Enhanced Virtual Assistant, rispondi in italiano.")

# ========= Client Ollama =========
ollama_client = Client(host=OLLAMA_BASE)

# ========= Utility =========
def log_to_file(question, bot_answer):
    try:
        with open(os.path.join(LOG_PATH, "log.txt"), "a", encoding="utf-8") as log_file:
            log_file.write(f"{_now()}\n[QUESTION]: {question};[OLLAMA]: {bot_answer}\n")
        if bot_answer:
            with open(os.path.join(LOG_PATH, "user.txt"), "a", encoding="utf-8") as bot_file:
                bot_file.write("user: " + str(question) + "\n")
                bot_file.write("bot: " + str(bot_answer) + "\n")
    except Exception as e:
        log_error(f"Errore scrivendo i log conversazione: {e}")

def split_string(msg):
    print(f"[DEBUG] Risposta grezza del modello: {msg}", file=sys.stderr)
    if isinstance(msg, str):
        return msg
    if isinstance(msg, dict) and "content" in msg:
        return str(msg["content"])
    return "(errore nella risposta del modello: contenuto non leggibile)"

def sanitize_response(text: str) -> str:
    """Rimuove i doppi asterischi ** (markdown bold)."""
    if not isinstance(text, str):
        return text
    text = text.replace("**", "")
    # opzionale: rimuovere anche __ e ``
    # text = text.replace("__", "").replace("`", "")
    return text

def check_ollama_connectivity(raise_on_fail=False):
    url = f"{OLLAMA_BASE.rstrip('/')}/api/tags"
    try:
        r = requests.get(url, timeout=5)
        r.raise_for_status()
        _ = r.json()
        log_info(f"Connessione a Ollama OK su {OLLAMA_BASE}")
        return True
    except Exception as e:
        log_error(
            f"Impossibile connettersi a Ollama su {OLLAMA_BASE} — {e}\n"
            "Verifica 'ollama serve', porta 11434 su 127.0.0.1 e valore 'ollama_host' nel config."
        )
        if raise_on_fail:
            raise
        return False

def get_response(messages, model_name: str = DEFAULT_MODEL):
    print(f"[DEBUG] Messaggi inviati al modello ({model_name}): {messages}", file=sys.stderr)
    try:
        start = time()
        response = ollama_client.chat(model=model_name, messages=messages)
        print(f"[DEBUG] Risposta completa ricevuta: {response}", file=sys.stderr)
        print(f"[DEBUG] Tempo di risposta del modello: {time()-start:.2f} secondi", file=sys.stderr)

        # Normalizzazione: supporta oggetto o dict
        content = None
        try:
            msg_obj = getattr(response, "message", None)
            if msg_obj is not None:
                content = getattr(msg_obj, "content", None)
        except Exception:
            pass
        if content is None and isinstance(response, dict):
            msg_dict = response.get("message")
            if isinstance(msg_dict, dict):
                content = msg_dict.get("content")
        if content is None and isinstance(response, dict) and "content" in response:
            content = response["content"]

        if not isinstance(content, str) or not content.strip():
            log_error("Formato risposta inatteso da Ollama: impossibile estrarre 'message.content'.")
            return {"content": "(errore: formato risposta inatteso da Ollama)"}
        return {"content": content}
    except Exception as e:
        log_error(f"Chiamata a Ollama fallita (host: {OLLAMA_BASE}, model: {model_name}) — {e}")
        return {"content": f"(errore: impossibile contattare Ollama su {OLLAMA_BASE} — {e})"}

# ========= Flask =========
app = Flask(__name__)
app.static_folder = 'static'

@app.route("/")
def home():
    try:
        url = f"{OLLAMA_BASE.rstrip('/')}/api/tags"
        r = requests.get(url, timeout=5)
        r.raise_for_status()
        model_list = r.json().get("models", [])
        model_names = [(m.get("name") or m.get("model")) for m in model_list if (m.get("name") or m.get("model"))]
        if not model_names:
            model_names = [DEFAULT_MODEL]
        print(f"[DEBUG] Modelli disponibili: {model_names}", file=sys.stderr)
    except Exception as e:
        log_error(f"Errore durante il recupero modelli da {OLLAMA_BASE} — {e}")
        model_names = [DEFAULT_MODEL]
    # se non hai template, rimuovi render_template e ritorna JSON semplice
    return render_template("indexollama.html", models=model_names)

@app.route("/get")
def get_bot_response():
    q = (request.args.get('msg') or '').strip()
    model = (request.args.get('model') or DEFAULT_MODEL).strip()
    messages = [{"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": q}]
    new_msg = get_response(messages, model)
    msgout = split_string(new_msg.get('content', new_msg))
    msgout = sanitize_response(msgout)
    log_to_file(q, msgout)
    return msgout

@app.route('/bot')
def bot():
    q = (request.args.get('query') or '').strip()
    model = (request.args.get('model') or DEFAULT_MODEL).strip()
    messages = [{"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": q}]
    new_msg = get_response(messages, model)
    msgout = split_string(new_msg.get('content', new_msg))
    msgout = sanitize_response(msgout)
    log_to_file(q, msgout)
    # send_to_ros2(msgout)  # se ti serve, reintroduci la funzione
    return msgout

@app.route('/json', methods=['GET', 'POST'])
def json_response():
    if request.method == 'POST':
        data = request.get_json(silent=True) or {}
        q = (data.get('query') or '').strip()
        model = (data.get('model') or DEFAULT_MODEL).strip()
    else:
        q = (request.args.get('query') or '').strip()
        model = (request.args.get('model') or DEFAULT_MODEL).strip()

    messages = [{"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": q}]
    new_msg = get_response(messages, model)
    msg = new_msg.get('content', new_msg)
    msg = sanitize_response(msg)
    return jsonify({"response": msg, "action": "ok"})

@app.route('/healthz')
def healthz():
    ok = check_ollama_connectivity(False)
    return ("ok", 200) if ok else ("ollama unreachable", 503)

if __name__ == '__main__':
    log_info(f"Avvio app-ollama | OLLAMA_BASE={OLLAMA_BASE} | DEFAULT_MODEL={DEFAULT_MODEL}")
    check_ollama_connectivity(False)
    app.run(host='0.0.0.0', debug=True, port=5000)
